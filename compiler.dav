// File: compiler.dav
// Author: David T.
// Description: Stage 1 compiler for the dav language.
// This file is compiled by the Stage 0 Python compiler.


// =============================================================
// Global Storage
// =============================================================

// --- Tokenizer Storage ---
beg char* token_types[1000];
beg int token_values[1000]; // Stores index into token_pool, or -1
beg int token_lines[1000];
beg int token_cols[1000];
beg char token_pool[50000]; // String pool for token values
beg int n_tokens = 0;      // Total number of tokens found


// --- Parser State ---
beg int parser_pos = 0; // Current token index for the parser


// =============================================================
// Function Declarations
// =============================================================

// --- Lexer Helpers ---
ah int is_letter(char c);
ah int is_digit(char c);
ah int is_space(char c);
ah int is_ident_char(char c);
ah char* check_keywords(char* s);
ah int add_simple_token(int index, char* type, int line, int col);

ah int tokenize(char* source_code);

// --- Parser Helpers ---
ah char* peek();
ah int next();
ah int expect(char* kind);

ah char* parse();
ah char* global_decl();
ah char* fn_decl();
ah char* let_stmt();
ah char* comment_stmt();


ah void print_token() {
    beg char* buffer = "";
    beg int i = 0;
    beg int j = 0;
    beg int pool_pos = 0;
    while j < n_tokens {
        pool_pos = token_values[j];
        i = i + strlen(token_types[j]) + 1;
        buffer = buffer + token_types[j] + " ";
        if pool_pos != -1 {
            while token_pool[pool_pos] != '\0' {
                buffer[i] = token_pool[pool_pos];
                pool_pos = pool_pos + 1;
                i = i + 1;
            }
            buffer[i] = ' ';
            i = i + 1;
        }
        j = j + 1;
    }
    buffer[i] = '\0';
    boo(buffer);
}


// =============================================================
// Main Entry Point
// =============================================================

ah int main() {
    // Test code with all global declaration types
    beg char* code = "// My Stage 1 Compiler\n\nbeg int global_var = 10;\n\nah int my_func();\n\nah int main() {\n    boo(global_var);\n}\n";
    
    // 1. Tokenize
    boo("--- Tokenizing ---");
    n_tokens = tokenize(code);
    boo("--- Tokenizing Complete ---");
    print_token();

    // 2. Parse
    boo("--- Parsing ---");
    beg char* c_code = parse();
    
    boo("--- Generated C Code ---");
    boo(c_code);
    
    return 0;
}


// =============================================================
// Parser
// =============================================================

ah char* parse() {
    // Main parser entry point.
    // Loops until EOF, parsing all global declarations.
    beg char* code_buffer = ""; // This will hold the entire generated C file

    while peek() != "EOF" {
        code_buffer = code_buffer + global_decl();
    }
    
    return code_buffer;
}

ah char* global_decl() {
    // Dispatches to the correct parser function
    // based on the next token.
    beg char* t = peek();

    if t == "FN" {
        return fn_decl();
    } else if t == "LET" {
        return let_stmt();
    } else if t == "COMMENT" {
        return comment_stmt();
    } else {
        // Error handling
        beg int tok_line = token_lines[parser_pos];
        boo("Error: Unexpected global token on line " + itos(tok_line));
        boo("Expected FN, LET, or COMMENT, but got: " + t);
        
        // Consume the bad token to prevent infinite loop
        next(); 
        return ""; // Return empty string for this declaration
    }
}

ah char* fn_decl() {
    boo("Parsing function (stub)...");
    
    // Consume the function prototype/definition
    // ah int my_func(int a) { ... }
    expect("FN");
    if peek() == "TYPE" { next(); }
    expect("ID");
    expect("LPAREN");
    while peek() != "RPAREN" {
        next(); // Consume params
    }
    expect("RPAREN");
    
    if peek() == "LBRACE" {
        // Function Definition
        expect("LBRACE");
        while (peek() != "RBRACE") {
            next(); // Consume body
        }
        expect("RBRACE");
    } else {
        // Function Declaration (Prototype)
        expect("SEMICOL");
    }

    return "/* C code for function (stub) */\n";
}

ah char* let_stmt() {
    boo("Parsing global variable (stub)...");
    
    // Consume the global variable
    // beg int x = 10;
    expect("LET");
    if peek() == "TYPE" { next(); }
    expect("ID");
    if peek() == "LSQUARE" {
        expect("LSQUARE");
        if peek() == "NUMBER" { next(); }
        expect("RSQUARE");
    }
    if peek() == "ASSIGN" {
        expect("ASSIGN");
        // Just consume one token for the value (e.g., NUMBER)
        next(); 
    }
    expect("SEMICOL");

    return "/* C code for global variable (stub) */\n";
}

ah char* comment_stmt() {
    next();
    return "";
}


// =============================================================
// Parser Helpers
// =============================================================

ah char* peek() {
    // Returns the type of the current token.
    // Automatically skips over any 'COMMENT' tokens.
    while token_types[parser_pos] == "COMMENT" {
        parser_pos = parser_pos + 1;
    }
    return token_types[parser_pos];
}

ah int next() {
    // Consumes the current token and returns its index.
    // Make sure to call peek() first to skip comments.
    
    // Skips any comments
    peek();
    beg int current_pos = parser_pos;
    parser_pos = parser_pos + 1;
    return current_pos;
}

ah int expect(char* kind) {
    // Checks if the current token is of the expected 'kind'.
    // If yes, consumes it and returns its index.
    // If no, prints an error and returns -1.
    
    // Skips comments and gets type
    beg char* tok_type = peek();
    
    if tok_type == kind {
        // Consume and return index
        return next();
    }
    
    // Handle error
    beg int tok_line = token_lines[parser_pos];
    boo("Error: Syntax Error on line " + itos(tok_line));
    boo("Expected token: " + kind);
    boo("... but got token: " + tok_type);
    
    // In a real compiler, we'd exit here.
    return -1; // Indicate error
}


// =============================================================
// Tokenizer
//
// This is the main lexer logic, ported from python/lexer/lexer.py
// =============================================================

ah int tokenize(char* source_code) {
    beg int pos = 0;
    beg int line_num = 1;
    beg int line_start = 0;
    
    beg char buffer[100];
    beg int i = 0;
 
    beg int token_count = 0;
    beg int pool_pos = 0;

    beg char c;
    beg int col;
    beg int j;
    beg char token_val;
    beg int token_start_col;
    
    while source_code[pos] != '\0' {
        c = source_code[pos];
        col = pos - line_start;
        i = 0;

        // --- 1. Skip Whitespace ---
        if is_space(c) {
            if c == '\n' {
                line_num = line_num + 1;
                line_start = pos + 1;
            }
            pos = pos + 1;
        } 
        
        // --- 2. Check for Numbers ---
        else if is_digit(c) {
            token_start_col = col;
            while is_digit(c) {
                buffer[i] = c; i = i + 1; pos = pos + 1; c = source_code[pos];
            }
            if c == '.' {
                buffer[i] = c; i = i + 1; pos = pos + 1; c = source_code[pos];
                while is_digit(c) {
                    buffer[i] = c; i = i + 1; pos = pos + 1; c = source_code[pos];
                }
            }
            buffer[i] = '\0';
            
            token_types[token_count] = "NUMBER";
            token_lines[token_count] = line_num;
            token_cols[token_count] = token_start_col;
            
            // Copy buffer to string pool
            token_values[token_count] = pool_pos;
            j = 0;

            // <= to include the '\0'
            while j <= i {
                token_pool[pool_pos] = buffer[j];
                pool_pos = pool_pos + 1;
                j = j + 1;
            }
            token_count = token_count + 1;
        }
        
        // --- 3. Check for Identifiers & Keywords ---
        else if is_letter(c) {
            token_start_col = col;
            while is_ident_char(c) {
                buffer[i] = c; i = i + 1; pos = pos + 1; c = source_code[pos];
            }
            buffer[i] = '\0';

            beg char* tok_type = check_keywords(buffer);
            token_types[token_count] = tok_type;
            token_lines[token_count] = line_num;
            token_cols[token_count] = token_start_col;

            // Copy buffer to string pool
            if tok_type != "ID" && tok_type != "TYPE" {
                token_values[token_count] = -1;
            } else {
                token_values[token_count] = pool_pos;
                j = 0;
                while j <= i {
                    token_pool[pool_pos] = buffer[j];
                    pool_pos = pool_pos + 1;
                    j = j + 1;
                }
            }
            token_count = token_count + 1;
        }

        // --- 4. Check for Multi-Char Tokens ---
        else if c == '=' {
            if source_code[pos + 1] == '=' {
                add_simple_token(token_count, "EQ", line_num, col);
                token_count = token_count + 1; pos = pos + 2;
            } else {
                add_simple_token(token_count, "ASSIGN", line_num, col);
                token_count = token_count + 1; pos = pos + 1;
            }
        }
        else if c == '!' && source_code[pos + 1] == '=' {
            add_simple_token(token_count, "NE", line_num, col);
            token_count = token_count + 1; pos = pos + 2;
        }
        else if c == '>' {
            if source_code[pos + 1] == '=' {
                add_simple_token(token_count, "GE", line_num, col);
                token_count = token_count + 1; pos = pos + 2;
            } else {
                add_simple_token(token_count, "GT", line_num, col);
                token_count = token_count + 1; pos = pos + 1;
            }
        }
        else if c == '<' {
            if source_code[pos + 1] == '=' {
                add_simple_token(token_count, "LE", line_num, col);
                token_count = token_count + 1; pos = pos + 2;
            } else {
                add_simple_token(token_count, "LT", line_num, col);
                token_count = token_count + 1; pos = pos + 1;
            }
        }
        else if c == '&' && source_code[pos + 1] == '&' {
            add_simple_token(token_count, "AND", line_num, col);
            token_count = token_count + 1; pos = pos + 2;
        }
        else if c == '|' && source_code[pos + 1] == '|' {
            add_simple_token(token_count, "OR", line_num, col);
            token_count = token_count + 1; pos = pos + 2;
        }
        else if c == '/' {
            if source_code[pos + 1] == '/' {
                add_simple_token(token_count, "COMMENT", line_num, col);
                token_count = token_count + 1; pos = pos + 2;
 
                // Loop to skip till after newline or EOL
                while source_code[pos] != '\n' {
                    pos = pos + 1;
                }
            } else {
                add_simple_token(token_count, "DIV", line_num, col);
                token_count = token_count + 1; pos = pos + 1;
            }
        }

        // --- 5. Check for Single-Char Tokens ---
        else if c == '(' {
            add_simple_token(token_count, "LPAREN", line_num, col);
            token_count = token_count + 1; pos = pos + 1;
        }
        else if c == ')' {
            add_simple_token(token_count, "RPAREN", line_num, col);
            token_count = token_count + 1; pos = pos + 1;
        }
        else if c == '{' {
            add_simple_token(token_count, "LBRACE", line_num, col);
            token_count = token_count + 1; pos = pos + 1;
        }
        else if c == '}' {
            add_simple_token(token_count, "RBRACE", line_num, col);
            token_count = token_count + 1; pos = pos + 1;
        }
        else if c == '[' {
            add_simple_token(token_count, "LSQUARE", line_num, col);
            token_count = token_count + 1; pos = pos + 1;
        }
        else if c == ']' {
            add_simple_token(token_count, "RSQUARE", line_num, col);
            token_count = token_count + 1; pos = pos + 1;
        }
        else if c == '+' {
            add_simple_token(token_count, "PLUS", line_num, col);
            token_count = token_count + 1; pos = pos + 1;
        }
        else if c == '-' {
            add_simple_token(token_count, "MINUS", line_num, col);
            token_count = token_count + 1; pos = pos + 1;
        }
        else if c == '*' {
            add_simple_token(token_count, "MUL", line_num, col);
            token_count = token_count + 1; pos = pos + 1;
        }
        else if c == ';' {
            add_simple_token(token_count, "SEMICOL", line_num, col);
            token_count = token_count + 1; pos = pos + 1;
        }
        else if c == ',' {
            add_simple_token(token_count, "COMMA", line_num, col);
            token_count = token_count + 1; pos = pos + 1;
        }

        // --- 6. Handle Strings and Chars ---
        else if c == '"' {
            token_start_col = col;
            pos = pos + 1; c = source_code[pos];

            while c != '"' && c != '\0' {
                if c == '\\' {
                    pos = pos + 1; c = source_code[pos];
                    
                    if c == 'n' { buffer[i] = '\n';
                    } else if c == 't' { buffer[i] = '\t';
                    } else if c == '"' { buffer[i] = '"';
                    } else if c == '\\' { buffer[i] = '\\';
                    } else { buffer[i] = c; }
                }
                else { buffer[i] = c; }
                i = i + 1; pos = pos + 1; c = source_code[pos];
            }
            
            // Check unclosed string
            if c == '\0' { boo("Error: Unclosed string literal!"); return 1; }
            pos = pos + 1;
            buffer[i] = '\0';

            // Add token
            token_types[token_count] = "STRING";
            token_lines[token_count] = line_num;
            token_cols[token_count] = token_start_col;
            token_values[token_count] = pool_pos;
            j = 0;
            while (j <= i) {
                token_pool[pool_pos] = buffer[j];
                pool_pos = pool_pos + 1;
                j = j + 1;
            }
            token_count = token_count + 1;
        }
        else if c == '\'' {
            token_start_col = col;
            pos = pos + 1; c = source_code[pos];
            token_val = c;

            if c == '\\' {
                pos = pos + 1; c = source_code[pos];

                if c == 'n' { token_val = '\n';
                } else if c == 't' { token_val = '\t';
                } else if c == '\'' { token_val = '\'';
                } else if c == '\\' { token_val = '\\';
                } else { token_val = c; }
            }

            pos = pos + 1; c = source_code[pos];
            if c != '\'' { boo("Error: Unclosed or invalid char literal!"); return 1; }
            pos = pos + 1;
            
            // Add token
            buffer[0] = token_val; buffer[1] = '\0';
            token_types[token_count] = "CHAR";
            token_lines[token_count] = line_num;
            token_cols[token_count] = token_start_col;
            token_values[token_count] = pool_pos;
            token_pool[pool_pos] = buffer[0];
            token_pool[pool_pos + 1] = buffer[1];
            pool_pos = pool_pos + 2;
            token_count = token_count + 1;
        }

        // --- 7. Handle Errors ---
        else {
            boo("Error: Unexpected character!");
            boo(ctos(c));
            return 1;
        }
    }
    
    // Add EOF Token
    add_simple_token(token_count, "EOF", line_num, col);
    token_count = token_count + 1;

    return token_count;
}

// =============================================================
// Lexer Helpers
//
// We port the logic from the Python lexer.
// =============================================================

ah int is_letter(char c) {
    // Checks if a character is a letter or underscore.
    // Corresponds to: [A-Za-z_]
    return (c >= 'a' && c <= 'z') || 
           (c >= 'A' && c <= 'Z') || 
           (c == '_');
}

ah int is_digit(char c) {
    // Checks if a character is a 0-9 digit.
    // Corresponds to: \d
    return (c >= '0' && c <= '9');
}

ah int is_space(char c) {
    // Checks for whitespace characters to skip.
    // Corresponds to: [ \t\n]
    return (c == ' ') || (c == '\t') || (c == '\n');
}

ah int is_ident_char(char c) {
    // Checks if a char can be part of an identifier *after* the first char.
    // Corresponds to: [A-Za-z0-9_]
    return is_letter(c) || is_digit(c);
}

ah char* check_keywords(char* s) {
    // Checks if a string 's' is a keyword.
    // If it is, return the keyword's Token Type.
    // Otherwise, return "ID".
    if s == "ah" {
        return "FN";
    } else if s == "beg" {
        return "LET";
    } else if s == "boo" {
        return "PRINT";
    } else if s == "if" {
        return "IF";
    } else if s == "else" {
        return "ELSE";
    } else if s == "while" {
        return "WHILE";
    } else if s == "return" {
        return "RETURN";
    } else if s == "int*" || s == "char*" ||
              s == "int" || s == "char" ||
              s == "void"  {
        return "TYPE";
    }

    // Default case: not a keyword
    return "ID";
}

// Helper to add a simple token (without a value) to the token arrays.
ah int add_simple_token(int index, char* type, int line, int col) {
    token_types[index] = type;
    token_values[index] = -1; // -1 means no value
    token_lines[index] = line;
    token_cols[index] = col;
    return 0;
}

