// File: compiler.dav
// Author: David T.
// Description: Stage 1 compiler for the dav language.
// This file is compiled by the Stage 0 Python compiler.


// =============================================================
// Global Storage
// =============================================================

// --- Tokenizer Storage ---
beg char* token_types[1000];
beg int token_values[1000]; // Stores index into token_pool, or -1
beg int token_lines[1000];
beg int token_cols[1000];
beg char token_pool[50000]; // String pool for token values
beg int n_tokens = 0;      // Total number of tokens found


// --- Parser State ---
beg int parser_pos = 0; // Current token index for the parser


// =============================================================
// Function Declarations
// =============================================================

// --- Lexer Helpers ---
ah int is_letter(char c);
ah int is_digit(char c);
ah int is_space(char c);
ah int is_ident_char(char c);
ah char* check_keywords(char* s);
ah int add_simple_token(int index, char* type, int line, int col);

ah int tokenize(char* source_code);

// --- Parser Helpers ---
ah char* parse();
ah char* global_decl();
ah char* fn_decl();
ah char* let_stmt(int is_global);
ah char* comment_stmt();

ah char* peek();
ah int next();
ah int expect(char* kind);

ah int clear_local_symbols();
ah char* get_symbol_type(int is_global, char* name);
ah int add_symbol(int is_global, char* name, char* type);

// --- Symbol Table Storage ---
// We store 'char*' pointers for names and types.
// The Stage 0 compiler will get the name strings from the token_pool.
// The type strings will be string literals (e.g., "int", "char*").

// Global Scope (self.env)
beg char* global_names[100];
beg char* global_types[100];
beg int n_globals = 0;

// Local Scope (self.variables)
beg char* local_names[100];
beg char* local_types[100];
beg int n_locals = 0;


ah void print_token() {
    beg char* buffer = "";
    beg int i = 0;
    beg int j = 0;
    beg int pool_pos = 0;
    while j < n_tokens {
        pool_pos = token_values[j];
        i = i + strlen(token_types[j]) + 1;
        buffer = buffer + token_types[j] + " ";
        if pool_pos != -1 {
            while token_pool[pool_pos] != '\0' {
                buffer[i] = token_pool[pool_pos];
                pool_pos = pool_pos + 1;
                i = i + 1;
            }
            buffer[i] = ' ';
            i = i + 1;
        }
        j = j + 1;
    }
    buffer[i] = '\0';
    boo(buffer);
}


// =============================================================
// Main Entry Point
// =============================================================

ah int main() {
    // Test code with all global declaration types
    beg char* code = "// My Stage 1 Compiler\n\nbeg int global_var = 10;\n\nah int my_func();\n\nah int main() {\n    boo(global_var);\n}\n";
    
    // 1. Tokenize
    boo("--- Tokenizing ---");
    n_tokens = tokenize(code);
    boo("--- Tokenizing Complete ---");
    print_token();

    // 2. Parse
    boo("--- Parsing ---");
    beg char* c_code = parse();
    
    boo("--- Generated C Code ---");
    boo(c_code);
    
    return 0;
}


// =============================================================
// Parser
// =============================================================

ah char* parse() {
    // Main parser entry point.
    // Loops until EOF, parsing all global declarations.
    beg char* code_buffer = ""; // This will hold the entire generated C file

    while peek() != "EOF" {
        code_buffer = code_buffer + global_decl();
    }
    
    return code_buffer;
}

ah char* global_decl() {
    // Dispatches to the correct parser function
    // based on the next token.
    beg char* tok = peek();

    if tok == "FN" {
        return fn_decl();
    } else if tok == "LET" {
        return let_stmt(1);
    } else if tok == "COMMENT" {
        return comment_stmt();
    } else {
        // Error handling
        beg int tok_line = token_lines[parser_pos];
        boo("Error: Unexpected global token on line " + itos(tok_line));
        boo("Expected FN, LET, or COMMENT, but got: " + tok);
        
        // Consume the bad token to prevent infinite loop
        next(); 
        return ""; // Return empty string for this declaration
    }
}

ah char* fn_decl() {
    // Consume the function prototype/definition
    // ah int my_func(int a) { ... }
    expect("FN");
    
    // --- Get Type ---
    beg char* fn_type = "int"; // Default type
    beg int fn_type_idx = -1;
    if peek() == "TYPE" {
        fn_type_idx = next();
        fn_type = token_pool + token_values[fn_type_idx];
    }
    
    // --- Get Name ---
    beg fn_name_idx = expect("ID");
    beg char* fn_name = token_pool + token_values[fn_name_idx];

    // --- Add to global ---
    add_symbol(1, fn_name, fn_type);

    expect("LPAREN");
    while peek() != "RPAREN" {
        next(); // Consume params
    }
    expect("RPAREN");
    
    // --- Inside the LBRACE branch ---
    if peek() == "LBRACE" {
        // Function Definition
        next();
        
        // --- Setup local scope ---
        clear_local_symbols();
        // TODO: Loop through params and add_symbol("local", ...)
        // ---
        
        beg char* body = "";
        while peek() != "RBRACE" {
            // TODO: This needs to call statement(), not next()
            next(); // Stub
        }
        expect("RBRACE");
        return "/* C code for function (stub) */\n";
    } else {
        // ... (prototype logic) ...
    }
    
    return "/* C code for function (stub) */\n";
}

ah char* let_stmt(int is_global) {
    beg int line_num = token_lines[parser_pos];
    expect("LET");

    // --- Get Type ---
    beg char* var_type = "int"; // Default type
    beg int var_type_idx = -1;
    if peek() == "TYPE" {
        var_type_idx = next();
        var_type = token_pool + token_values[var_type_idx];
    }

    // --- Get Name ---
    beg var_name_idx = expect("ID");
    beg char* var_name = token_pool + token_values[var_name_idx];

    // Check redefinition
    if (is_global == 0 && get_symbol_type(0, var_name) != "") ||
       (is_global == 1 && get_symbol_type(1, var_name) != "") {
        
        boo("Error: Redefinition of variable");
        boo(var_name);
        return ""; // Error
    }

    // --- Parsing Cases ---
    if peek() == "ASSIGN" {
        // --- Case 1: Declaration with Assignment (e.g., beg x = 10) ---
        next();
        
        // We will need expr() later. For now, just consume one token.
        // TODO: Replace this with call to expr()
        beg int val_tok = next(); // Stub: consume value
        beg char* rhs_expr = token_pool + token_values[val_tok];
        // END TODO
        
        // TODO: Add type checking logic from Python
        
        expect("SEMICOL");
        add_symbol(is_global, var_name, var_type);
        
        // C code: e.g., "int x = 5;"
        return var_type + " " + var_name + " = " + rhs_expr + ";\n";
    }
    else if peek() == "LSQUARE" {
        // --- Case 2: Array Declaration (e.g., beg int arr[10]) ---
        next();

        if var_type == "int" {
            boo("Error: Array declaration must have an explicit type on line" + itos(line_num));
            return "";
        }
        
        beg int size_tok = expect("NUMBER");
        beg char* size = token_pool + token_values[size_tok];
        
        expect("RSQUARE");
        expect("SEMICOL");

        // Store array type as 'base_type*' (e.g., 'int*')
        beg char* array_type = var_type + "*";
        add_symbol(is_global, var_name, array_type);
        
        // C code: e.g., "int arr[10];"
        return var_type + " " + var_name + "[" + size + "];\n";
    }
    else if peek() == "SEMICOL" {
        // --- Case 3: Declaration without Assignment (e.g., beg int x;) ---
        next();
        
        if var_type == "int" {
            boo("Error: Declaration without assignment must have explicit type on line" + itos(line_num));
            return "";
        }
        
        add_symbol(is_global, var_name, var_type);
        
        // C code: e.g., "int x;"
        return var_type + " " + var_name + ";\n";
    }
    else {
        boo("Error: Expected '=', '[', or ';' after variable name on line" + itos(line_num));
        next(); // Consume bad token
        return "";
    }
}

ah char* comment_stmt() {
    next();
    return "";
}


// =============================================================
// Parser Helpers
// =============================================================

ah char* peek() {
    // Returns the type of the current token.
    // Automatically skips over any 'COMMENT' tokens.
    while token_types[parser_pos] == "COMMENT" {
        parser_pos = parser_pos + 1;
    }
    return token_types[parser_pos];
}

ah int next() {
    // Consumes the current token and returns its index.
    // Make sure to call peek() first to skip comments.
    
    // Skips any comments
    peek();
    beg int current_pos = parser_pos;
    parser_pos = parser_pos + 1;
    return current_pos;
}

ah int expect(char* kind) {
    // Checks if the current token is of the expected 'kind'.
    // If yes, consumes it and returns its index.
    // If no, prints an error and returns -1.
    
    // Skips comments and gets type
    beg char* tok_type = peek();
    
    if tok_type == kind {
        // Consume and return index
        return next();
    }
    
    // Handle error
    beg int tok_line = token_lines[parser_pos];
    boo("Error: Syntax Error on line " + itos(tok_line));
    boo("Expected token: " + kind);
    boo("... but got token: " + tok_type);
    
    // In a real compiler, we'd exit here.
    return -1; // Indicate error
}


// =============================================================
// Symbol Table Helpers
// =============================================================

ah int clear_local_symbols() {
    // Clears the local (function-level) symbol table.
    // Called when entering a new function.
    n_locals = 0;
    return 0;
}

ah char* get_symbol_type(int is_global, char* name) {
    // Searches for a variable 'name' in the given 'scope'.
    // Returns its type (e.g., "int", "char*") if found.
    // Returns "" (empty string) if not found.
    
    beg int i = 0;
    
    if is_global == 0 {
        while i < n_locals {
            if local_names[i] == name {
                return local_types[i];
            }
            i = i + 1;
        }
    } else { // "global"
        while i < n_globals {
            if global_names[i] == name {
                return global_types[i];
            }
            i = i + 1;
        }
    }
    
    // Not found, check outer scope (if local)
    if is_global == 0 {
        return get_symbol_type(1, name);
    }
    
    return ""; // Not found anywhere
}

ah int add_symbol(int is_global, char* name, char* type) {
    // Adds a new variable to the symbol table.
    // Returns 0 on success.
    // NOTE: This function assumes you have already checked for redefinition.
    if is_global == 0 {
        local_names[n_locals] = name;
        local_types[n_locals] = type;
        n_locals = n_locals + 1;
    } else {
        global_names[n_globals] = name;
        global_types[n_globals] = type;
        n_globals = n_globals + 1;
    }
    return 0;
}


// =============================================================
// Tokenizer
//
// This is the main lexer logic, ported from python/lexer/lexer.py
// =============================================================

ah int tokenize(char* source_code) {
    beg int pos = 0;
    beg int line_num = 1;
    beg int line_start = 0;
    
    beg char buffer[100];
    beg int i = 0;
 
    beg int token_count = 0;
    beg int pool_pos = 0;

    beg char c;
    beg int col;
    beg int j;
    beg char token_val;
    beg int token_start_col;
    
    while source_code[pos] != '\0' {
        c = source_code[pos];
        col = pos - line_start;
        i = 0;

        // --- 1. Skip Whitespace ---
        if is_space(c) {
            if c == '\n' {
                line_num = line_num + 1;
                line_start = pos + 1;
            }
            pos = pos + 1;
        } 
        
        // --- 2. Check for Numbers ---
        else if is_digit(c) {
            token_start_col = col;
            while is_digit(c) {
                buffer[i] = c; i = i + 1; pos = pos + 1; c = source_code[pos];
            }
            if c == '.' {
                buffer[i] = c; i = i + 1; pos = pos + 1; c = source_code[pos];
                while is_digit(c) {
                    buffer[i] = c; i = i + 1; pos = pos + 1; c = source_code[pos];
                }
            }
            buffer[i] = '\0';
            
            token_types[token_count] = "NUMBER";
            token_lines[token_count] = line_num;
            token_cols[token_count] = token_start_col;
            
            // Copy buffer to string pool
            token_values[token_count] = pool_pos;
            j = 0;

            // <= to include the '\0'
            while j <= i {
                token_pool[pool_pos] = buffer[j];
                pool_pos = pool_pos + 1;
                j = j + 1;
            }
            token_count = token_count + 1;
        }
        
        // --- 3. Check for Identifiers & Keywords ---
        else if is_letter(c) {
            token_start_col = col;
            while is_ident_char(c) {
                buffer[i] = c; i = i + 1; pos = pos + 1; c = source_code[pos];
            }
            buffer[i] = '\0';

            beg char* tok_type = check_keywords(buffer);
            token_types[token_count] = tok_type;
            token_lines[token_count] = line_num;
            token_cols[token_count] = token_start_col;

            // Copy buffer to string pool
            if tok_type != "ID" && tok_type != "TYPE" {
                token_values[token_count] = -1;
            } else {
                token_values[token_count] = pool_pos;
                j = 0;
                while j <= i {
                    token_pool[pool_pos] = buffer[j];
                    pool_pos = pool_pos + 1;
                    j = j + 1;
                }
            }
            token_count = token_count + 1;
        }

        // --- 4. Check for Multi-Char Tokens ---
        else if c == '=' {
            if source_code[pos + 1] == '=' {
                add_simple_token(token_count, "EQ", line_num, col);
                token_count = token_count + 1; pos = pos + 2;
            } else {
                add_simple_token(token_count, "ASSIGN", line_num, col);
                token_count = token_count + 1; pos = pos + 1;
            }
        }
        else if c == '!' && source_code[pos + 1] == '=' {
            add_simple_token(token_count, "NE", line_num, col);
            token_count = token_count + 1; pos = pos + 2;
        }
        else if c == '>' {
            if source_code[pos + 1] == '=' {
                add_simple_token(token_count, "GE", line_num, col);
                token_count = token_count + 1; pos = pos + 2;
            } else {
                add_simple_token(token_count, "GT", line_num, col);
                token_count = token_count + 1; pos = pos + 1;
            }
        }
        else if c == '<' {
            if source_code[pos + 1] == '=' {
                add_simple_token(token_count, "LE", line_num, col);
                token_count = token_count + 1; pos = pos + 2;
            } else {
                add_simple_token(token_count, "LT", line_num, col);
                token_count = token_count + 1; pos = pos + 1;
            }
        }
        else if c == '&' && source_code[pos + 1] == '&' {
            add_simple_token(token_count, "AND", line_num, col);
            token_count = token_count + 1; pos = pos + 2;
        }
        else if c == '|' && source_code[pos + 1] == '|' {
            add_simple_token(token_count, "OR", line_num, col);
            token_count = token_count + 1; pos = pos + 2;
        }
        else if c == '/' {
            if source_code[pos + 1] == '/' {
                add_simple_token(token_count, "COMMENT", line_num, col);
                token_count = token_count + 1; pos = pos + 2;
 
                // Loop to skip till after newline or EOL
                while source_code[pos] != '\n' {
                    pos = pos + 1;
                }
            } else {
                add_simple_token(token_count, "DIV", line_num, col);
                token_count = token_count + 1; pos = pos + 1;
            }
        }

        // --- 5. Check for Single-Char Tokens ---
        else if c == '(' {
            add_simple_token(token_count, "LPAREN", line_num, col);
            token_count = token_count + 1; pos = pos + 1;
        }
        else if c == ')' {
            add_simple_token(token_count, "RPAREN", line_num, col);
            token_count = token_count + 1; pos = pos + 1;
        }
        else if c == '{' {
            add_simple_token(token_count, "LBRACE", line_num, col);
            token_count = token_count + 1; pos = pos + 1;
        }
        else if c == '}' {
            add_simple_token(token_count, "RBRACE", line_num, col);
            token_count = token_count + 1; pos = pos + 1;
        }
        else if c == '[' {
            add_simple_token(token_count, "LSQUARE", line_num, col);
            token_count = token_count + 1; pos = pos + 1;
        }
        else if c == ']' {
            add_simple_token(token_count, "RSQUARE", line_num, col);
            token_count = token_count + 1; pos = pos + 1;
        }
        else if c == '+' {
            add_simple_token(token_count, "PLUS", line_num, col);
            token_count = token_count + 1; pos = pos + 1;
        }
        else if c == '-' {
            add_simple_token(token_count, "MINUS", line_num, col);
            token_count = token_count + 1; pos = pos + 1;
        }
        else if c == '*' {
            add_simple_token(token_count, "MUL", line_num, col);
            token_count = token_count + 1; pos = pos + 1;
        }
        else if c == ';' {
            add_simple_token(token_count, "SEMICOL", line_num, col);
            token_count = token_count + 1; pos = pos + 1;
        }
        else if c == ',' {
            add_simple_token(token_count, "COMMA", line_num, col);
            token_count = token_count + 1; pos = pos + 1;
        }

        // --- 6. Handle Strings and Chars ---
        else if c == '"' {
            token_start_col = col;
            pos = pos + 1; c = source_code[pos];

            while c != '"' && c != '\0' {
                if c == '\\' {
                    pos = pos + 1; c = source_code[pos];
                    
                    if c == 'n' { buffer[i] = '\n';
                    } else if c == 't' { buffer[i] = '\t';
                    } else if c == '"' { buffer[i] = '"';
                    } else if c == '\\' { buffer[i] = '\\';
                    } else { buffer[i] = c; }
                }
                else { buffer[i] = c; }
                i = i + 1; pos = pos + 1; c = source_code[pos];
            }
            
            // Check unclosed string
            if c == '\0' { boo("Error: Unclosed string literal!"); return 1; }
            pos = pos + 1;
            buffer[i] = '\0';

            // Add token
            token_types[token_count] = "STRING";
            token_lines[token_count] = line_num;
            token_cols[token_count] = token_start_col;
            token_values[token_count] = pool_pos;
            j = 0;
            while (j <= i) {
                token_pool[pool_pos] = buffer[j];
                pool_pos = pool_pos + 1;
                j = j + 1;
            }
            token_count = token_count + 1;
        }
        else if c == '\'' {
            token_start_col = col;
            pos = pos + 1; c = source_code[pos];
            token_val = c;

            if c == '\\' {
                pos = pos + 1; c = source_code[pos];

                if c == 'n' { token_val = '\n';
                } else if c == 't' { token_val = '\t';
                } else if c == '\'' { token_val = '\'';
                } else if c == '\\' { token_val = '\\';
                } else { token_val = c; }
            }

            pos = pos + 1; c = source_code[pos];
            if c != '\'' { boo("Error: Unclosed or invalid char literal!"); return 1; }
            pos = pos + 1;
            
            // Add token
            buffer[0] = token_val; buffer[1] = '\0';
            token_types[token_count] = "CHAR";
            token_lines[token_count] = line_num;
            token_cols[token_count] = token_start_col;
            token_values[token_count] = pool_pos;
            token_pool[pool_pos] = buffer[0];
            token_pool[pool_pos + 1] = buffer[1];
            pool_pos = pool_pos + 2;
            token_count = token_count + 1;
        }

        // --- 7. Handle Errors ---
        else {
            boo("Error: Unexpected character!");
            boo(ctos(c));
            return 1;
        }
    }
    
    // Add EOF Token
    add_simple_token(token_count, "EOF", line_num, col);
    token_count = token_count + 1;

    return token_count;
}

// =============================================================
// Lexer Helpers
//
// We port the logic from the Python lexer.
// =============================================================

ah int is_letter(char c) {
    // Checks if a character is a letter or underscore.
    // Corresponds to: [A-Za-z_]
    return (c >= 'a' && c <= 'z') || 
           (c >= 'A' && c <= 'Z') || 
           (c == '_');
}

ah int is_digit(char c) {
    // Checks if a character is a 0-9 digit.
    // Corresponds to: \d
    return (c >= '0' && c <= '9');
}

ah int is_space(char c) {
    // Checks for whitespace characters to skip.
    // Corresponds to: [ \t\n]
    return (c == ' ') || (c == '\t') || (c == '\n');
}

ah int is_ident_char(char c) {
    // Checks if a char can be part of an identifier *after* the first char.
    // Corresponds to: [A-Za-z0-9_]
    return is_letter(c) || is_digit(c);
}

ah char* check_keywords(char* s) {
    // Checks if a string 's' is a keyword.
    // If it is, return the keyword's Token Type.
    // Otherwise, return "ID".
    if s == "ah" {
        return "FN";
    } else if s == "beg" {
        return "LET";
    } else if s == "boo" {
        return "PRINT";
    } else if s == "if" {
        return "IF";
    } else if s == "else" {
        return "ELSE";
    } else if s == "while" {
        return "WHILE";
    } else if s == "return" {
        return "RETURN";
    } else if s == "int*" || s == "char*" ||
              s == "int" || s == "char" ||
              s == "void"  {
        return "TYPE";
    }

    // Default case: not a keyword
    return "ID";
}

// Helper to add a simple token (without a value) to the token arrays.
ah int add_simple_token(int index, char* type, int line, int col) {
    token_types[index] = type;
    token_values[index] = -1; // -1 means no value
    token_lines[index] = line;
    token_cols[index] = col;
    return 0;
}

