// File: compiler.dav
// Author: David T.
// Description: Stage 1 compiler for the dav language.
// This file is compiled by the Stage 0 Python compiler.


// =============================================================
// Global Storage
// =============================================================

// --- Tokenizer Storage ---
beg char* token_types[1000];
beg int token_values[1000]; // Stores index into token_pool, or -1
beg int token_lines[1000];
beg int token_cols[1000];
beg char token_pool[50000]; // String pool for token values
beg int n_tokens = 0;      // Total number of tokens found


// --- Parser State ---
beg int parser_pos = 0; // Current token index for the parser


// =============================================================
// Function Declarations
// =============================================================

ah int is_letter(char c);
ah int is_digit(char c);
ah int is_space(char c);
ah int is_ident_char(char c);
ah char* check_keywords(char* s);
ah int add_simple_token(int index, char* type, int line, int col);

ah char* peek();
ah int next();
ah int expect(char* kind);

ah int tokenize(char* source_code);
ah int parse(char* c_code, char* token_types[1000],
             int token_values[1000], int token_lines[1000], 
             int token_cols[1000], char token_pool[50000]);

// =============================================================
// Main Entry Point
// =============================================================

ah int main() {
    // Test code with a comment
    beg char* code = "ah main() { // test comment\n beg x = 5; }";
    
    // 1. Tokenize
    // Tokenize populates the global arrays and returns the count
    n_tokens = tokenize(code);
    
    // 2. Parse (Test)
    boo("--- Starting Parse ---");
    
    // Test the new parser helpers
    expect("FN");
    expect("ID");
    expect("LPAREN");
    expect("RPAREN");
    expect("LBRACE");
    
    // The peek() inside expect() will automatically skip the comment!
    
    expect("LET");
    expect("ID");
    expect("ASSIGN");
    expect("NUMBER");
    expect("SEMICOL");
    expect("RBRACE");
    expect("EOF");

    boo("--- Parse Complete ---");
    boo("SUCCESS: Parser helpers work.");
    
    return 0;
}


// =============================================================
// Parser Helpers
// =============================================================

ah char* peek() {
    // Returns the type of the current token.
    // Automatically skips over any 'COMMENT' tokens.
    while token_types[parser_pos] == "COMMENT" {
        parser_pos = parser_pos + 1;
    }
    return token_types[parser_pos];
}

ah int next() {
    // Consumes the current token and returns its index.
    // Make sure to call peek() first to skip comments.
    
    // Skips any comments
    peek();
    beg int current_pos = parser_pos;
    parser_pos = parser_pos + 1;
    return current_pos;
}

ah int expect(char* kind) {
    // Checks if the current token is of the expected 'kind'.
    // If yes, consumes it and returns its index.
    // If no, prints an error and returns -1.
    
    // Skips comments and gets type
    beg char* tok_type = peek();
    
    if tok_type == kind {
        // Consume and return index
        return next();
    }
    
    // Handle error
    beg int tok_line = token_lines[parser_pos];
    boo("Error: Syntax Error on line");
    boo(itos(tok_line));
    boo("Expected token:");
    boo(kind);
    boo("... but got token:");
    boo(tok_type);
    
    // In a real compiler, we'd exit here.
    return -1; // Indicate error
}


// =============================================================
// Tokenizer
//
// This is the main lexer logic, ported from python/lexer/lexer.py
// =============================================================

ah int tokenize(char* source_code) {
    beg int pos = 0;
    beg int line_num = 1;
    beg int line_start = 0;
    
    beg char buffer[100];
    beg int i = 0;
 
    beg int token_count = 0;
    beg int pool_pos = 0;

    beg char c;
    beg int col;
    beg int j;
    beg char token_val;
    beg int token_start_col;
    
    while source_code[pos] != '\0' {
        c = source_code[pos];
        col = pos - line_start;
        i = 0;

        // --- 1. Skip Whitespace ---
        if is_space(c) {
            if c == '\n' {
                line_num = line_num + 1;
                line_start = pos + 1;
            }
            pos = pos + 1;
        } 
        
        // --- 2. Check for Numbers ---
        else if is_digit(c) {
            token_start_col = col;
            while is_digit(c) {
                buffer[i] = c; i = i + 1; pos = pos + 1; c = source_code[pos];
            }
            if c == '.' {
                buffer[i] = c; i = i + 1; pos = pos + 1; c = source_code[pos];
                while is_digit(c) {
                    buffer[i] = c; i = i + 1; pos = pos + 1; c = source_code[pos];
                }
            }
            buffer[i] = '\0';
            
            token_types[token_count] = "NUMBER";
            token_lines[token_count] = line_num;
            token_cols[token_count] = token_start_col;
            
            // Copy buffer to string pool
            token_values[token_count] = pool_pos;
            j = 0;

            // <= to include the '\0'
            while j <= i {
                token_pool[pool_pos] = buffer[j];
                pool_pos = pool_pos + 1;
                j = j + 1;
            }
            token_count = token_count + 1;
        }
        
        // --- 3. Check for Identifiers & Keywords ---
        else if is_letter(c) {
            token_start_col = col;
            while is_ident_char(c) {
                buffer[i] = c; i = i + 1; pos = pos + 1; c = source_code[pos];
            }
            buffer[i] = '\0';

            token_types[token_count] = check_keywords(buffer);
            token_lines[token_count] = line_num;
            token_cols[token_count] = token_start_col;

            // Copy buffer to string pool
            token_values[token_count] = pool_pos;
            j = 0;
            while j <= i {
                token_pool[pool_pos] = buffer[j];
                pool_pos = pool_pos + 1;
                j = j + 1;
            }
            token_count = token_count + 1;
        }

        // --- 4. Check for Single-Char Tokens ---
        else if c == '(' {
            add_simple_token(token_count, "LPAREN", line_num, col);
            token_count = token_count + 1; pos = pos + 1;
        }
        else if c == ')' {
            add_simple_token(token_count, "RPAREN", line_num, col);
            token_count = token_count + 1; pos = pos + 1;
        }
        else if c == '{' {
            add_simple_token(token_count, "LBRACE", line_num, col);
            token_count = token_count + 1; pos = pos + 1;
        }
        else if c == '}' {
            add_simple_token(token_count, "RBRACE", line_num, col);
            token_count = token_count + 1; pos = pos + 1;
        }
        else if c == '[' {
            add_simple_token(token_count, "LSQUARE", line_num, col);
            token_count = token_count + 1; pos = pos + 1;
        }
        else if c == ']' {
            add_simple_token(token_count, "RSQUARE", line_num, col);
            token_count = token_count + 1; pos = pos + 1;
        }
        else if c == '+' {
            add_simple_token(token_count, "PLUS", line_num, col);
            token_count = token_count + 1; pos = pos + 1;
        }
        else if c == '-' {
            add_simple_token(token_count, "MINUS", line_num, col);
            token_count = token_count + 1; pos = pos + 1;
        }
        else if c == '*' {
            add_simple_token(token_count, "MUL", line_num, col);
            token_count = token_count + 1; pos = pos + 1;
        }
        else if c == '/' {
            add_simple_token(token_count, "DIV", line_num, col);
            token_count = token_count + 1; pos = pos + 1;
        }
        else if c == ';' {
            add_simple_token(token_count, "SEMICOL", line_num, col);
            token_count = token_count + 1; pos = pos + 1;
        }
        else if c == ',' {
            add_simple_token(token_count, "COMMA", line_num, col);
            token_count = token_count + 1; pos = pos + 1;
        }

        // --- 5. Check for Multi-Char Tokens ---
        else if c == '=' {
            if source_code[pos + 1] == '=' {
                add_simple_token(token_count, "EQ", line_num, col);
                token_count = token_count + 1; pos = pos + 2;
            } else {
                add_simple_token(token_count, "ASSIGN", line_num, col);
                token_count = token_count + 1; pos = pos + 1;
            }
        }
        else if c == '!' && source_code[pos + 1] == '=' {
            add_simple_token(token_count, "NE", line_num, col);
            token_count = token_count + 1; pos = pos + 2;
        }
        else if c == '>' {
            if source_code[pos + 1] == '=' {
                add_simple_token(token_count, "GE", line_num, col);
                token_count = token_count + 1; pos = pos + 2;
            } else {
                add_simple_token(token_count, "GT", line_num, col);
                token_count = token_count + 1; pos = pos + 1;
            }
        }
        else if c == '<' {
            if source_code[pos + 1] == '=' {
                add_simple_token(token_count, "LE", line_num, col);
                token_count = token_count + 1; pos = pos + 2;
            } else {
                add_simple_token(token_count, "LT", line_num, col);
                token_count = token_count + 1; pos = pos + 1;
            }
        }
        else if c == '&' && source_code[pos + 1] == '&' {
            add_simple_token(token_count, "AND", line_num, col);
            token_count = token_count + 1; pos = pos + 2;
        }
        else if c == '|' && source_code[pos + 1] == '|' {
            add_simple_token(token_count, "OR", line_num, col);
            token_count = token_count + 1; pos = pos + 2;
        }

        // --- 6. Handle Strings and Chars ---
        else if c == '"' {
            token_start_col = col;
            pos = pos + 1; c = source_code[pos];

            while c != '"' && c != '\0' {
                if c == '\\' {
                    pos = pos + 1; c = source_code[pos];
                    
                    if c == 'n' { buffer[i] = '\n';
                    } else if c == 't' { buffer[i] = '\t';
                    } else if c == '"' { buffer[i] = '"';
                    } else if c == '\\' { buffer[i] = '\\';
                    } else { buffer[i] = c; }
                }
                else { buffer[i] = c; }
                i = i + 1; pos = pos + 1; c = source_code[pos];
            }
            
            // Check unclosed string
            if c == '\0' { boo("Error: Unclosed string literal!"); return 1; }
            pos = pos + 1;
            buffer[i] = '\0';

            // Add token
            token_types[token_count] = "STRING";
            token_lines[token_count] = line_num;
            token_cols[token_count] = token_start_col;
            token_values[token_count] = pool_pos;
            j = 0;
            while (j <= i) {
                token_pool[pool_pos] = buffer[j];
                pool_pos = pool_pos + 1;
                j = j + 1;
            }
            token_count = token_count + 1;
        }
        else if c == '\'' {
            token_start_col = col;
            pos = pos + 1; c = source_code[pos];
            token_val = c;

            if c == '\\' {
                pos = pos + 1; c = source_code[pos];

                if c == 'n' { token_val = '\n';
                } else if c == 't' { token_val = '\t';
                } else if c == '\'' { token_val = '\'';
                } else if c == '\\' { token_val = '\\';
                } else { token_val = c; }
            }

            pos = pos + 1; c = source_code[pos];
            if c != '\'' { boo("Error: Unclosed or invalid char literal!"); return 1; }
            pos = pos + 1;
            
            // Add token
            buffer[0] = token_val; buffer[1] = '\0';
            token_types[token_count] = "CHAR";
            token_lines[token_count] = line_num;
            token_cols[token_count] = token_start_col;
            token_values[token_count] = pool_pos;
            token_pool[pool_pos] = buffer[0];
            token_pool[pool_pos + 1] = buffer[1];
            pool_pos = pool_pos + 2;
            token_count = token_count + 1;
        }

        // --- 7. Handle Errors ---
        else {
            boo("Error: Unexpected character!");
            boo(ctos(c));
            return 1;
        }
    }
    
    // Add EOF Token
    add_simple_token(token_count, "EOF", line_num, col);
    token_count = token_count + 1;

    return token_count;
}

// =============================================================
// Lexer Helpers
//
// We port the logic from the Python lexer.
// =============================================================

ah int is_letter(char c) {
    // Checks if a character is a letter or underscore.
    // Corresponds to: [A-Za-z_]
    return (c >= 'a' && c <= 'z') || 
           (c >= 'A' && c <= 'Z') || 
           (c == '_');
}

ah int is_digit(char c) {
    // Checks if a character is a 0-9 digit.
    // Corresponds to: \d
    return (c >= '0' && c <= '9');
}

ah int is_space(char c) {
    // Checks for whitespace characters to skip.
    // Corresponds to: [ \t\n]
    return (c == ' ') || (c == '\t') || (c == '\n');
}

ah int is_ident_char(char c) {
    // Checks if a char can be part of an identifier *after* the first char.
    // Corresponds to: [A-Za-z0-9_]
    return is_letter(c) || is_digit(c);
}

ah char* check_keywords(char* s) {
    // Checks if a string 's' is a keyword.
    // If it is, return the keyword's Token Type.
    // Otherwise, return "ID".
    if s == "ah" {
        return "FN";
    }
    else if s == "beg" {
        return "LET";
    }
    else if s == "boo" {
        return "PRINT";
    }
    else if s == "if" {
        return "IF";
    }
    else if s == "else" {
        return "ELSE";
    }
    else if s == "while" {
        return "WHILE";
    }
    else if s == "return" {
        return "RETURN";
    }

    // Default case: not a keyword
    return "ID";
}

// Helper to add a simple token (without a value) to the token arrays.
ah int add_simple_token(int index, char* type, int line, int col) {
    token_types[index] = type;
    token_values[index] = -1; // -1 means no value
    token_lines[index] = line;
    token_cols[index] = col;
    return 0;
}

